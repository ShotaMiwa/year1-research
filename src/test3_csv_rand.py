import os
import json
import torch
import numpy as np
import segeval
from tqdm import tqdm
from transformers import set_seed
from model import SegModel
from decimal import Decimal
import csv  
import matplotlib.pyplot as plt  

# ===========================================================
# è¨­å®šéƒ¨åˆ†
# ===========================================================
INFERENCE_DATA_BASE_DIR = "./inference_data"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SAVE_PATH = "./results"
os.makedirs(SAVE_PATH, exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
MODEL_CONFIGS = {
    "trained-model": {
        "coherence_model": "cl-tohoku/bert-base-japanese",
        "topic_model": "pkshatech/simcse-ja-bert-base-clcmlp",
        "inference_data_path": f"{INFERENCE_DATA_BASE_DIR}/default/inference_data.json",
        "model_checkpoint": "/content/drive/MyDrive/seg_models/hiroyuki_model/epoch_4_step_918",  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        "use_comments_for_topic": True,  # ãƒ†ã‚¹ãƒˆæ™‚ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨
        "fusion_method": "average"  # å¹³å‡èåˆã®ã¿ã‚’ä½¿ç”¨
    }
}

# ===========================================================
# æ·±åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
# ===========================================================
def depth_score_cal(scores):
    """æ·±åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    scores_array = np.array(scores)
    output_scores = []
    
    for i in range(len(scores_array)):
        lflag = scores_array[i]
        rflag = scores_array[i]
        
        if i == 0:
            for r in range(i+1, len(scores_array)):
                if rflag <= scores_array[r]:
                    rflag = scores_array[r]
                else:
                    break
        elif i == len(scores_array)-1:
            for l in range(i-1, -1, -1):
                if lflag <= scores_array[l]:
                    lflag = scores_array[l]
                else:
                    break
        else:
            for r in range(i+1, len(scores_array)):
                if rflag <= scores_array[r]:
                    rflag = scores_array[r]
                else:
                    break
            for l in range(i-1, -1, -1):
                if lflag <= scores_array[l]:
                    lflag = scores_array[l]
                else:
                    break
        
        depth_score = 0.5 * (lflag + rflag - 2 * scores_array[i])
        output_scores.append(depth_score)
    
    return output_scores

# ===========================================================
# å¢ƒç•Œæ¤œå‡ºé–¢æ•°
# ===========================================================
def detect_boundaries(depth_scores, method='adaptive', num_boundaries=None, threshold=0.5):
    """æ·±åº¦ã‚¹ã‚³ã‚¢ã‹ã‚‰å¢ƒç•Œã‚’æ¤œå‡º"""
    depth_scores = np.array(depth_scores)
    
    if method == 'adaptive':
        mean_score = np.mean(depth_scores)
        std_score = np.std(depth_scores)
        threshold = mean_score + 0.5 * std_score
        boundaries = np.where(depth_scores > threshold)[0]
        
    elif method == 'fixed':
        if num_boundaries is None:
            num_boundaries = max(1, len(depth_scores) // 20)
        boundaries = np.argsort(depth_scores)[-num_boundaries:]
        
    elif method == 'threshold':
        boundaries = np.where(depth_scores > threshold)[0]
    
    else:
        raise ValueError("Unknown method")
    
    return sorted(boundaries)

# ===========================================================
# ã‚«ã‚¹ã‚¿ãƒ JSONã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
# ===========================================================
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (Decimal, np.integer)):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        return super().default(obj)

# ===========================================================
# CSVä¿å­˜é–¢æ•°
# ===========================================================
def save_results_to_csv(result, save_path):
    """å¢ƒç•Œæ¨å®šçµæœã‚’CSVã«ä¿å­˜"""
    csv_path = f"{save_path}/boundary_results.csv"
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
        writer.writerow([
            'sentence_index', 
            'sentence', 
            'raw_score', 
            'depth_score', 
            'predicted_boundary', 
            'gold_boundary',
            'is_correct'
        ])
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for i, (sentence, raw_score, depth_score, pred, gold) in enumerate(zip(
            result['sentences'],
            result['raw_scores'],
            result['depth_scores'],
            result['predicted_boundaries'],
            result['gold_boundaries']
        )):
            is_correct = "æ­£è§£" if pred == gold else "ä¸æ­£è§£"
            writer.writerow([
                i,
                sentence,
                f"{raw_score:.6f}",
                f"{depth_score:.6f}",
                pred,
                gold,
                is_correct
            ])
    
    print(f"å¢ƒç•Œæ¨å®šçµæœã‚’CSVã«ä¿å­˜: {csv_path}")

# ===========================================================
# ã‚¹ã‚³ã‚¢ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ é–¢æ•°
# ===========================================================
def create_score_histograms(result, save_path):
    """å„ç¨®ã‚¹ã‚³ã‚¢ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆ"""
    debug_scores = result.get('debug_scores', {})
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    hist_data = {}
    
    # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ç”Ÿã‚¹ã‚³ã‚¢
    if 'coherence_raw_scores' in debug_scores and debug_scores['coherence_raw_scores']:
        hist_data['Coherence Raw Scores'] = debug_scores['coherence_raw_scores']
    
    # ãƒˆãƒ”ãƒƒã‚¯ç”Ÿã‚¹ã‚³ã‚¢
    if 'topic_raw_scores' in debug_scores and debug_scores['topic_raw_scores']:
        hist_data['Topic Raw Scores'] = debug_scores['topic_raw_scores']
    
    # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢
    if 'coherence_normalized_scores' in debug_scores and debug_scores['coherence_normalized_scores']:
        hist_data['Coherence Normalized Scores'] = debug_scores['coherence_normalized_scores']
    
    # ãƒˆãƒ”ãƒƒã‚¯æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢
    if 'topic_normalized_scores' in debug_scores and debug_scores['topic_normalized_scores']:
        hist_data['Topic Normalized Scores'] = debug_scores['topic_normalized_scores']
    
    # æœ€çµ‚ç”Ÿã‚¹ã‚³ã‚¢
    if 'final_raw' in debug_scores and debug_scores['final_raw']:
        hist_data['Final Raw Scores'] = debug_scores['final_raw']
    
    # sigmoidå¾Œæœ€çµ‚ã‚¹ã‚³ã‚¢
    if 'raw_scores' in result and result['raw_scores']:
        hist_data['Final Sigmoid Scores'] = result['raw_scores']
    
    # æ·±åº¦ã‚¹ã‚³ã‚¢
    if 'depth_scores' in result and result['depth_scores']:
        hist_data['Depth Scores'] = result['depth_scores']
    
    if not hist_data:
        print("!!!ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç”¨ã®ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“!!!")
        return
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®ä½œæˆ
    num_plots = len(hist_data)
    fig, axes = plt.subplots((num_plots + 2) // 3, 3, figsize=(18, 4 * ((num_plots + 2) // 3)))
    
    # 1æ¬¡å…ƒé…åˆ—ã«å¤‰æ›ï¼ˆã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆãŒ1è¡Œã®å ´åˆï¼‰
    if num_plots <= 3:
        axes = axes.reshape(1, -1)
    
    colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive']
    
    for idx, (title, scores) in enumerate(hist_data.items()):
        row = idx // 3
        col = idx % 3
        
        ax = axes[row, col] if num_plots > 3 else axes[col]
        
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®æç”»
        n, bins, patches = ax.hist(scores, bins=30, alpha=0.7, color=colors[idx % len(colors)], edgecolor='black')
        
        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        mean_val = np.mean(scores)
        std_val = np.std(scores)
        median_val = np.median(scores)
        
        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ—ãƒ­ãƒƒãƒˆã«è¿½åŠ 
        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.4f}')
        ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.4f}')
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«
        ax.set_title(f'{title}\nDistribution', fontsize=12, fontweight='bold')
        ax.set_xlabel('Score Value', fontsize=10)
        ax.set_ylabel('Frequency', fontsize=10)
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        
        # çµ±è¨ˆæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã§è¡¨ç¤º
        stats_text = f'Mean: {mean_val:.4f}\nStd: {std_val:.4f}\nMedian: {median_val:.4f}\nMin: {min(scores):.4f}\nMax: {max(scores):.4f}'
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8), fontsize=8)
    
    # æœªä½¿ç”¨ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for idx in range(num_plots, axes.size):
        row = idx // 3
        col = idx % 3
        if num_plots > 3:
            axes[row, col].set_visible(False)
        else:
            axes[col].set_visible(False)
    
    plt.tight_layout()
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä¿å­˜
    hist_path = f"{save_path}/score_histograms.png"
    plt.savefig(hist_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ã‚¹ã‚³ã‚¢ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä¿å­˜: {hist_path}")

# ===========================================================
# ã‚¹ã‚³ã‚¢å¯è¦–åŒ–é–¢æ•°
# ===========================================================
def visualize_scores(result, save_path):
    """ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ç”Ÿã‚¹ã‚³ã‚¢ã€ãƒˆãƒ”ãƒƒã‚¯ç”Ÿã‚¹ã‚³ã‚¢ã€æ­£è¦åŒ–å¾Œã®ã‚¹ã‚³ã‚¢ã‚’å¯è¦–åŒ–"""
    # ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    debug_scores = result.get('debug_scores', {})
    
    # å„ç¨®ã‚¹ã‚³ã‚¢ã‚’å–å¾—
    coherence_raw = debug_scores.get('coherence_raw_scores', [])
    topic_raw = debug_scores.get('topic_raw_scores', [])
    coherence_normalized = debug_scores.get('coherence_normalized_scores', [])
    topic_normalized = debug_scores.get('topic_normalized_scores', [])
    
    if not coherence_raw and not topic_raw and not coherence_normalized and not topic_normalized:
        print("âš ï¸ å¯è¦–åŒ–ç”¨ã®ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # æ–‡ã®æ•°ã«å¿œã˜ã¦ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã‚’èª¿æ•´
    num_sentences = len(result['raw_scores'])
    
    # æ–‡ã®æ•°ã«åŸºã¥ã„ã¦å‹•çš„ã«ã‚µã‚¤ã‚ºèª¿æ•´
    if num_sentences <= 50:
        fig_width = 12
        font_size = 10
    elif num_sentences <= 100:
        fig_width = 16
        font_size = 9
    elif num_sentences <= 200:
        fig_width = 20
        font_size = 8
    else:
        fig_width = 24
        font_size = 7
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®æ•°ã‚’æ±ºå®šï¼ˆæ­£è¦åŒ–å¾Œã®ã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆã¯5ã¤ã€ãªã„å ´åˆã¯3ã¤ï¼‰
    has_normalized = coherence_normalized and topic_normalized
    num_subplots = 5 if has_normalized else 3
    fig_height = 4 * num_subplots  # å„ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®é«˜ã•ã‚’4ã‚¤ãƒ³ãƒã«è¨­å®š
    
    line_width = 1.5 if num_sentences <= 100 else 1.0
    
    # ã‚°ãƒ©ãƒ•ã®ä½œæˆ
    plt.figure(figsize=(fig_width, fig_height))
    
    # æ–‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    x = range(num_sentences)
    
    subplot_idx = 1
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ç”Ÿã‚¹ã‚³ã‚¢
    if coherence_raw:
        plt.subplot(num_subplots, 1, subplot_idx)
        plt.plot(x, coherence_raw[:num_sentences], 'b-', 
                label='Coherence Raw Scores', linewidth=line_width)
        plt.title('Coherence Raw Scores by Sentence Order', fontsize=font_size+2)
        plt.xlabel('Sentence Index', fontsize=font_size)
        plt.ylabel('Score', fontsize=font_size)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=font_size)
        subplot_idx += 1
        
        # xè»¸ã®ç›®ç››ã‚Šé–“éš”ã‚’èª¿æ•´
        if num_sentences > 50:
            plt.xticks(range(0, num_sentences, max(1, num_sentences//20)))
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: ãƒˆãƒ”ãƒƒã‚¯ç”Ÿã‚¹ã‚³ã‚¢
    if topic_raw:
        plt.subplot(num_subplots, 1, subplot_idx)
        plt.plot(x, topic_raw[:num_sentences], 'g-', 
                label='Topic Raw Scores', linewidth=line_width)
        plt.title('Topic Raw Scores by Sentence Order', fontsize=font_size+2)
        plt.xlabel('Sentence Index', fontsize=font_size)
        plt.ylabel('Score', fontsize=font_size)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=font_size)
        subplot_idx += 1
        
        # xè»¸ã®ç›®ç››ã‚Šé–“éš”ã‚’èª¿æ•´
        if num_sentences > 50:
            plt.xticks(range(0, num_sentences, max(1, num_sentences//20)))
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ï¼ˆè¿½åŠ ï¼‰
    if coherence_normalized:
        plt.subplot(num_subplots, 1, subplot_idx)
        plt.plot(x, coherence_normalized[:num_sentences], 'c-', 
                label='Coherence Normalized Scores', linewidth=line_width)
        plt.title('Coherence Normalized Scores by Sentence Order', fontsize=font_size+2)
        plt.xlabel('Sentence Index', fontsize=font_size)
        plt.ylabel('Score', fontsize=font_size)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=font_size)
        subplot_idx += 1
        
        # xè»¸ã®ç›®ç››ã‚Šé–“éš”ã‚’èª¿æ•´
        if num_sentences > 50:
            plt.xticks(range(0, num_sentences, max(1, num_sentences//20)))
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: ãƒˆãƒ”ãƒƒã‚¯æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ï¼ˆè¿½åŠ ï¼‰
    if topic_normalized:
        plt.subplot(num_subplots, 1, subplot_idx)
        plt.plot(x, topic_normalized[:num_sentences], 'y-', 
                label='Topic Normalized Scores', linewidth=line_width)
        plt.title('Topic Normalized Scores by Sentence Order', fontsize=font_size+2)
        plt.xlabel('Sentence Index', fontsize=font_size)
        plt.ylabel('Score', fontsize=font_size)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=font_size)
        subplot_idx += 1
        
        # xè»¸ã®ç›®ç››ã‚Šé–“éš”ã‚’èª¿æ•´
        if num_sentences > 50:
            plt.xticks(range(0, num_sentences, max(1, num_sentences//20)))
    
    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ5: æœ€çµ‚ã‚¹ã‚³ã‚¢ã¨æ·±åº¦ã‚¹ã‚³ã‚¢
    plt.subplot(num_subplots, 1, subplot_idx)
    plt.plot(x, result['raw_scores'], 'r-', 
            label='Final Scores', linewidth=line_width, alpha=0.7)
    plt.plot(x, result['depth_scores'], 'm-', 
            label='Depth Scores', linewidth=line_width, alpha=0.7)
    
    # äºˆæ¸¬å¢ƒç•Œã‚’ç¸¦ç·šã§è¡¨ç¤º
    pred_boundaries = [i for i, val in enumerate(result['predicted_boundaries']) if val == 1]
    for boundary in pred_boundaries:
        plt.axvline(x=boundary, color='red', linestyle='--', alpha=0.7, linewidth=1)
    
    # æ­£è§£å¢ƒç•Œã‚’ç¸¦ç·šã§è¡¨ç¤º
    gold_boundaries = [i for i, val in enumerate(result['gold_boundaries']) if val == 1]
    for boundary in gold_boundaries:
        plt.axvline(x=boundary, color='green', linestyle='--', alpha=0.7, linewidth=1)
    
    # å‡¡ä¾‹ã«å¢ƒç•Œæƒ…å ±ã‚’è¿½åŠ 
    from matplotlib.lines import Line2D
    legend_elements = [
        Line2D([0], [0], color='r', lw=2, alpha=0.7, label='Final Scores'),
        Line2D([0], [0], color='m', lw=2, alpha=0.7, label='Depth Scores'),
        Line2D([0], [0], color='red', linestyle='--', lw=1, label='Predicted Boundaries'),
        Line2D([0], [0], color='green', linestyle='--', lw=1, label='Gold Boundaries')
    ]
    
    plt.title('Final Scores and Depth Scores with Boundaries', fontsize=font_size+2)
    plt.xlabel('Sentence Index', fontsize=font_size)
    plt.ylabel('Score', fontsize=font_size)
    plt.grid(True, alpha=0.3)
    plt.legend(handles=legend_elements, fontsize=font_size)
    
    # xè»¸ã®ç›®ç››ã‚Šé–“éš”ã‚’èª¿æ•´
    if num_sentences > 50:
        plt.xticks(range(0, num_sentences, max(1, num_sentences//20)))
    
    plt.tight_layout()
    
    # ã‚°ãƒ©ãƒ•ã‚’é«˜è§£åƒåº¦ã§ä¿å­˜
    plot_path = f"{save_path}/score_visualization.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ã‚¹ã‚³ã‚¢å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {plot_path}")
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆ
    create_score_histograms(result, save_path)
    
    # æ–‡æ•°ãŒå¤šã„å ´åˆã¯è¿½åŠ ã®ã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã‚‚ä½œæˆ
    if num_sentences > 100:
        create_zoomed_plots(result, save_path, num_sentences, has_normalized)

def create_zoomed_plots(result, save_path, num_sentences, has_normalized=False):
    """æ–‡æ•°ãŒå¤šã„å ´åˆã«ã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã—ãŸã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
    debug_scores = result.get('debug_scores', {})
    coherence_raw = debug_scores.get('coherence_raw_scores', [])
    topic_raw = debug_scores.get('topic_raw_scores', [])
    coherence_normalized = debug_scores.get('coherence_normalized_scores', [])
    topic_normalized = debug_scores.get('topic_normalized_scores', [])
    
    # ã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã™ã‚‹åŒºé–“ã‚’è¨­å®šï¼ˆä¾‹: 50æ–‡ã”ã¨ï¼‰
    chunk_size = 50
    num_chunks = (num_sentences + chunk_size - 1) // chunk_size
    
    for chunk_idx in range(num_chunks):
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, num_sentences)
        
        if end_idx - start_idx < 10:  # å°ã•ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
            
        # ã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã®ä½œæˆ
        num_subplots = 5 if has_normalized else 3
        plt.figure(figsize=(12, 4 * num_subplots))
        
        x_zoom = range(start_idx, end_idx)
        subplot_idx = 1
        
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ç”Ÿã‚¹ã‚³ã‚¢ï¼ˆã‚ºãƒ¼ãƒ ã‚¤ãƒ³ï¼‰
        if coherence_raw:
            plt.subplot(num_subplots, 1, subplot_idx)
            plt.plot(x_zoom, coherence_raw[start_idx:end_idx], 'b-', 
                    label='Coherence Raw Scores', linewidth=1.5)
            plt.title(f'Coherence Raw Scores (Sentences {start_idx}-{end_idx-1})', fontsize=12)
            plt.xlabel('Sentence Index', fontsize=10)
            plt.ylabel('Score', fontsize=10)
            plt.grid(True, alpha=0.3)
            plt.legend(fontsize=10)
            subplot_idx += 1
        
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: ãƒˆãƒ”ãƒƒã‚¯ç”Ÿã‚¹ã‚³ã‚¢ï¼ˆã‚ºãƒ¼ãƒ ã‚¤ãƒ³ï¼‰
        if topic_raw:
            plt.subplot(num_subplots, 1, subplot_idx)
            plt.plot(x_zoom, topic_raw[start_idx:end_idx], 'g-', 
                    label='Topic Raw Scores', linewidth=1.5)
            plt.title(f'Topic Raw Scores (Sentences {start_idx}-{end_idx-1})', fontsize=12)
            plt.xlabel('Sentence Index', fontsize=10)
            plt.ylabel('Score', fontsize=10)
            plt.grid(True, alpha=0.3)
            plt.legend(fontsize=10)
            subplot_idx += 1
        
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ï¼ˆã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã€è¿½åŠ ï¼‰
        if coherence_normalized:
            plt.subplot(num_subplots, 1, subplot_idx)
            plt.plot(x_zoom, coherence_normalized[start_idx:end_idx], 'c-', 
                    label='Coherence Normalized Scores', linewidth=1.5)
            plt.title(f'Coherence Normalized Scores (Sentences {start_idx}-{end_idx-1})', fontsize=12)
            plt.xlabel('Sentence Index', fontsize=10)
            plt.ylabel('Score', fontsize=10)
            plt.grid(True, alpha=0.3)
            plt.legend(fontsize=10)
            subplot_idx += 1
        
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: ãƒˆãƒ”ãƒƒã‚¯æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ï¼ˆã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã€è¿½åŠ ï¼‰
        if topic_normalized:
            plt.subplot(num_subplots, 1, subplot_idx)
            plt.plot(x_zoom, topic_normalized[start_idx:end_idx], 'y-', 
                    label='Topic Normalized Scores', linewidth=1.5)
            plt.title(f'Topic Normalized Scores (Sentences {start_idx}-{end_idx-1})', fontsize=12)
            plt.xlabel('Sentence Index', fontsize=10)
            plt.ylabel('Score', fontsize=10)
            plt.grid(True, alpha=0.3)
            plt.legend(fontsize=10)
            subplot_idx += 1
        
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ5: æœ€çµ‚ã‚¹ã‚³ã‚¢ã¨æ·±åº¦ã‚¹ã‚³ã‚¢ï¼ˆã‚ºãƒ¼ãƒ ã‚¤ãƒ³ï¼‰
        plt.subplot(num_subplots, 1, subplot_idx)
        plt.plot(x_zoom, result['raw_scores'][start_idx:end_idx], 'r-', 
                label='Final Scores', linewidth=1.5, alpha=0.7)
        plt.plot(x_zoom, result['depth_scores'][start_idx:end_idx], 'm-', 
                label='Depth Scores', linewidth=1.5, alpha=0.7)
        
        # å¢ƒç•Œç·šï¼ˆã‚ºãƒ¼ãƒ ã‚¤ãƒ³ç¯„å›²å†…ã®ã¿è¡¨ç¤ºï¼‰
        pred_boundaries = [i for i, val in enumerate(result['predicted_boundaries']) 
                          if val == 1 and start_idx <= i < end_idx]
        for boundary in pred_boundaries:
            plt.axvline(x=boundary, color='red', linestyle='--', alpha=0.7, linewidth=1)
        
        gold_boundaries = [i for i, val in enumerate(result['gold_boundaries']) 
                          if val == 1 and start_idx <= i < end_idx]
        for boundary in gold_boundaries:
            plt.axvline(x=boundary, color='green', linestyle='--', alpha=0.7, linewidth=1)
        
        plt.title(f'Final and Depth Scores (Sentences {start_idx}-{end_idx-1})', fontsize=12)
        plt.xlabel('Sentence Index', fontsize=10)
        plt.ylabel('Score', fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=10)
        
        plt.tight_layout()
        
        # ã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
        zoom_plot_path = f"{save_path}/score_visualization_zoom_{start_idx}_{end_idx-1}.png"
        plt.savefig(zoom_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ã‚ºãƒ¼ãƒ ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {zoom_plot_path}")

# ===========================================================
# å˜ä¸€ãƒ¢ãƒ‡ãƒ«æ¨è«–é–¢æ•°
# ===========================================================
def run_inference_for_model(model_name, model_config):
    """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã®æ¨è«–å®Ÿè¡Œ"""
    print(f"\n{'='*60}")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print(f"{'='*60}")
    
    # æ¨è«–ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    possible_paths = [
        model_config["inference_data_path"],
        f"{INFERENCE_DATA_BASE_DIR}/{model_name}/inference_data_{model_name}.json",
        f"{INFERENCE_DATA_BASE_DIR}/{model_name}/inference_data.json"
    ]
    
    inference_data_path = None
    for path in possible_paths:
        if os.path.exists(path):
            inference_data_path = path
            break
    
    if inference_data_path is None:
        print(f"âŒ æ¨è«–ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        for path in possible_paths:
            print(f"  - {path}")
        return None
    
    gold_labels_path = f"{INFERENCE_DATA_BASE_DIR}/gold_labels.json"
    
    print(f"æ¨è«–ãƒ‡ãƒ¼ã‚¿: {inference_data_path}")
    
    # æ¨è«–ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with open(inference_data_path, "r", encoding="utf-8") as f:
        inference_data = json.load(f)
    
    with open(gold_labels_path, "r", encoding="utf-8") as f:
        gold_data = json.load(f)
    
    # ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
    coheren_inputs = inference_data["coheren_inputs"]
    coheren_masks = inference_data["coheren_masks"]
    coheren_types = inference_data["coheren_types"]
    topic_inputs = inference_data["topic_inputs"]
    topic_masks = inference_data["topic_masks"]
    comment_vectors = inference_data["comment_vectors"]
    sentences = inference_data["sentences"]
    
    boundary_labels = gold_data["boundary_labels"]
    
    # å‹•ç”»å¢ƒç•Œæƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°å‹•ç”»å¯¾å¿œï¼‰
    video_count = gold_data.get("video_count", 1)
    
    print(f"ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"  æ–‡ç« æ•°: {len(sentences)}")
    print(f"  å¢ƒç•Œæ•°: {sum(boundary_labels)}")
    print(f"  å‹•ç”»æ•°: {video_count}")
    print(f"  ãƒãƒƒãƒæ•°: {len(coheren_inputs)}")
    print(f"  ã‚³ãƒ¡ãƒ³ãƒˆä½¿ç”¨ãƒ•ãƒ©ã‚°: {model_config.get('use_comments_for_topic', True)}")
    print(f"  èåˆæ–¹æ³•: {model_config.get('fusion_method', 'average')}")
    
    # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    coheren_inputs_tensor = [torch.tensor(x) for x in coheren_inputs]
    coheren_masks_tensor = [torch.tensor(x) for x in coheren_masks]
    coheren_types_tensor = [torch.tensor(x) for x in coheren_types]
    comment_vectors_tensor = [torch.tensor(x) for x in comment_vectors]
    
    # =======================================================
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œorãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ï¼‰
    # =======================================================
    print("ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã®å‡¦ç†
        if "model_checkpoint" in model_config and os.path.exists(model_config["model_checkpoint"]):
            print(f"å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_config['model_checkpoint']}")
            model = SegModel(
                use_pretrained_only=False,  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                coherence_model_name=model_config["coherence_model"],
                topic_model_name=model_config["topic_model"],
                use_comments_for_topic=model_config.get("use_comments_for_topic", True),  # ã‚³ãƒ¡ãƒ³ãƒˆä½¿ç”¨ãƒ•ãƒ©ã‚°
                fusion_method=model_config.get("fusion_method", "average")  # å¹³å‡èåˆã®ã¿
            )
            # å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
            model.load_state_dict(torch.load(model_config["model_checkpoint"], map_location=DEVICE))
            print("âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        else:
            # é€šå¸¸ã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
            model = SegModel(
                use_pretrained_only=False,
                coherence_model_name=model_config["coherence_model"],
                topic_model_name=model_config["topic_model"],
                use_comments_for_topic=model_config.get("use_comments_for_topic", True),  # ã‚³ãƒ¡ãƒ³ãƒˆä½¿ç”¨ãƒ•ãƒ©ã‚°
                fusion_method=model_config.get("fusion_method", "average")  # å¹³å‡èåˆã®ã¿
            )
            print(f"âœ… äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_config['coherence_model']}, {model_config['topic_model']}")
        
        model.to(DEVICE)
        model.eval()
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    # =======================================================
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§æ¨è«–å®Ÿè¡Œ
    # =======================================================
    print("å…¨ãƒ‡ãƒ¼ã‚¿æ¨è«–å®Ÿè¡Œä¸­...")

    # ã¾ãšå…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’åé›†
    print("ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆåé›†ä¸­...")
    all_coherence_raw = []
    all_topic_raw = []

    with torch.no_grad():
        for i in tqdm(range(len(coheren_inputs_tensor)), desc="çµ±è¨ˆåé›†"):
            try:
                # coherenãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                coheren_input = coheren_inputs_tensor[i].unsqueeze(0).to(DEVICE)
                coheren_mask = coheren_masks_tensor[i].unsqueeze(0).to(DEVICE)
                coheren_type = coheren_types_tensor[i].unsqueeze(0).to(DEVICE)
                
                # topicãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                topic_input_0 = torch.tensor(topic_inputs[i][0]).unsqueeze(0).to(DEVICE)
                topic_input_1 = torch.tensor(topic_inputs[i][1]).unsqueeze(0).to(DEVICE)
                topic_mask_0 = torch.tensor(topic_masks[i][0]).unsqueeze(0).to(DEVICE)
                topic_mask_1 = torch.tensor(topic_masks[i][1]).unsqueeze(0).to(DEVICE)
                
                topic_i = [topic_input_0, topic_input_1]
                topic_m = [topic_mask_0, topic_mask_1]
                
                # ã‚³ãƒ¡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã®æº–å‚™
                if i < len(comment_vectors_tensor) - 1:
                    topic_comments = [
                        comment_vectors_tensor[i].unsqueeze(0).to(DEVICE),
                        comment_vectors_tensor[i+1].unsqueeze(0).to(DEVICE)
                    ]
                else:
                    topic_comments = [
                        comment_vectors_tensor[i].unsqueeze(0).to(DEVICE),
                        comment_vectors_tensor[i].unsqueeze(0).to(DEVICE)
                    ]
                
                # Topic_numã®æº–å‚™
                topic_num = [[1], [1]]
                
                # çµ±è¨ˆåé›†ç”¨ã®æ¨è«–ï¼ˆZ-scoreæ­£è¦åŒ–ãªã—ï¼‰
                s = model.infer(
                    coheren_input, coheren_mask, coheren_type, 
                    topic_i, topic_m, topic_comments, topic_num,
                    use_comments_for_topic=model_config.get("use_comments_for_topic", True),
                    fusion_method=model_config.get("fusion_method", "average")
                )
                
                # ç”Ÿã‚¹ã‚³ã‚¢ã‚’åé›†
                if hasattr(model, 'last_inference_debug_info'):
                    debug_info = model.last_inference_debug_info
                    if 'coherence_raw' in debug_info:
                        all_coherence_raw.extend(debug_info['coherence_raw'])
                    if 'topic_raw' in debug_info:
                        all_topic_raw.extend(debug_info['topic_raw'])
                        
            except Exception as e:
                print(f"\nçµ±è¨ˆåé›†: ãƒãƒƒãƒ {i} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã‚’è¨ˆç®—
    global_coherence_mean = np.mean(all_coherence_raw) if all_coherence_raw else 0.0
    global_coherence_std = np.std(all_coherence_raw) if all_coherence_raw else 1.0
    global_topic_mean = np.mean(all_topic_raw) if all_topic_raw else 0.0
    global_topic_std = np.std(all_topic_raw) if all_topic_raw else 1.0

    print(f"ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆ:")
    print(f"  ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹: mean={global_coherence_mean:.6f}, std={global_coherence_std:.6f}")
    print(f"  ãƒˆãƒ”ãƒƒã‚¯: mean={global_topic_mean:.6f}, std={global_topic_std:.6f}")

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã‚’ä½¿ç”¨ã—ã¦æœ¬æ¨è«–ã‚’å®Ÿè¡Œ
    print("æœ¬æ¨è«–å®Ÿè¡Œä¸­...")
    scores = []
    nan_count = 0

    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚¹ã‚³ã‚¢ç¯„å›²ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†
    coherence_raw_scores = []
    topic_raw_scores = []
    final_raw_scores = []
    coherence_normalized_scores = []
    topic_normalized_scores = []
    final_sigmoid_scores = []  # sigmoidå¾Œã®ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 

    with torch.no_grad():
        for i in tqdm(range(len(coheren_inputs_tensor)), desc=f"{model_name} æ¨è«–"):
            try:
                # coherenãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                coheren_input = coheren_inputs_tensor[i].unsqueeze(0).to(DEVICE)
                coheren_mask = coheren_masks_tensor[i].unsqueeze(0).to(DEVICE)
                coheren_type = coheren_types_tensor[i].unsqueeze(0).to(DEVICE)
                
                # topicãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                topic_input_0 = torch.tensor(topic_inputs[i][0]).unsqueeze(0).to(DEVICE)
                topic_input_1 = torch.tensor(topic_inputs[i][1]).unsqueeze(0).to(DEVICE)
                topic_mask_0 = torch.tensor(topic_masks[i][0]).unsqueeze(0).to(DEVICE)
                topic_mask_1 = torch.tensor(topic_masks[i][1]).unsqueeze(0).to(DEVICE)
                
                topic_i = [topic_input_0, topic_input_1]
                topic_m = [topic_mask_0, topic_mask_1]
                
                # ã‚³ãƒ¡ãƒ³ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã®æº–å‚™
                if i < len(comment_vectors_tensor) - 1:
                    topic_comments = [
                        comment_vectors_tensor[i].unsqueeze(0).to(DEVICE),
                        comment_vectors_tensor[i+1].unsqueeze(0).to(DEVICE)
                    ]
                else:
                    topic_comments = [
                        comment_vectors_tensor[i].unsqueeze(0).to(DEVICE),
                        comment_vectors_tensor[i].unsqueeze(0).to(DEVICE)
                    ]
                
                # Topic_numã®æº–å‚™
                topic_num = [[1], [1]]
                
                # ãƒ¢ãƒ‡ãƒ«æ¨è«–ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã‚’æ¸¡ã™ï¼‰
                s = model.infer(
                    coheren_input, coheren_mask, coheren_type, 
                    topic_i, topic_m, topic_comments, topic_num,
                    use_comments_for_topic=model_config.get("use_comments_for_topic", True),
                    fusion_method=model_config.get("fusion_method", "average"),  # å¹³å‡èåˆã®ã¿
                    global_coherence_mean=global_coherence_mean,
                    global_coherence_std=global_coherence_std,
                    global_topic_mean=global_topic_mean,
                    global_topic_std=global_topic_std
                )
                
                # ã‚¹ã‚³ã‚¢ã®å‡¦ç†
                processed_scores = []
                for score in s:
                    if torch.is_tensor(score):
                        score_val = score.item()
                    else:
                        score_val = score
                    
                    if np.isnan(score_val) or np.isinf(score_val):
                        score_val = 0.5
                        nan_count += 1
                    
                    processed_scores.append(score_val)
                
                scores.extend(processed_scores)
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šç”Ÿã‚¹ã‚³ã‚¢ç¯„å›²ã®åé›†
                if hasattr(model, 'last_inference_debug_info'):
                    debug_info = model.last_inference_debug_info
                    if 'coherence_raw' in debug_info:
                        coherence_raw_scores.extend(debug_info['coherence_raw'])
                    if 'topic_raw' in debug_info:
                        topic_raw_scores.extend(debug_info['topic_raw'])
                    if 'final_raw' in debug_info:
                        final_raw_scores.extend(debug_info['final_raw'])
                    if 'coherence_normalized' in debug_info:
                        coherence_normalized_scores.extend(debug_info['coherence_normalized'])
                    if 'topic_normalized' in debug_info:
                        topic_normalized_scores.extend(debug_info['topic_normalized'])
                
            except Exception as e:
                print(f"\nãƒãƒƒãƒ {i} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å‰ã®ãƒãƒƒãƒã®ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                if scores:
                    default_scores = [scores[-1]] * 1
                else:
                    default_scores = [0.5] * 1
                scores.extend(default_scores)
                continue

    # sigmoidå¾Œã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆæœ€çµ‚å‡ºåŠ›ã‚¹ã‚³ã‚¢ï¼‰
    final_sigmoid_scores = scores  # model.infer()ã§ã™ã§ã«sigmoidã‚’é€šã—ãŸå€¤ãŒè¿”ã•ã‚Œã¦ã„ã‚‹

    # =======================================================
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šã‚¹ã‚³ã‚¢ç¯„å›²ã®å‡ºåŠ›
    # =======================================================
    print(f"\nğŸ” ã‚¹ã‚³ã‚¢ç¯„å›²ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
    if coherence_raw_scores:
        coherence_arr = np.array(coherence_raw_scores)
        print(f"  ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ç”Ÿã‚¹ã‚³ã‚¢ç¯„å›²: min={coherence_arr.min():.6f}, max={coherence_arr.max():.6f}, mean={coherence_arr.mean():.6f}")
    else:
        print(f"  ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ç”Ÿã‚¹ã‚³ã‚¢: ãƒ‡ãƒ¼ã‚¿ãªã—")

    if topic_raw_scores:
        topic_arr = np.array(topic_raw_scores)
        print(f"  ãƒˆãƒ”ãƒƒã‚¯ç”Ÿã‚¹ã‚³ã‚¢ç¯„å›²: min={topic_arr.min():.6f}, max={topic_arr.max():.6f}, mean={topic_arr.mean():.6f}")
    else:
        print(f"  ãƒˆãƒ”ãƒƒã‚¯ç”Ÿã‚¹ã‚³ã‚¢: ãƒ‡ãƒ¼ã‚¿ãªã—")

    # æ­£è¦åŒ–å¾Œã®ã‚¹ã‚³ã‚¢ç¯„å›²ã‚’è¿½åŠ 
    if coherence_normalized_scores:
        coherence_norm_arr = np.array(coherence_normalized_scores)
        print(f"  ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ç¯„å›²: min={coherence_norm_arr.min():.6f}, max={coherence_norm_arr.max():.6f}, mean={coherence_norm_arr.mean():.6f}")
    else:
        print(f"  ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢: ãƒ‡ãƒ¼ã‚¿ãªã—")

    if topic_normalized_scores:
        topic_norm_arr = np.array(topic_normalized_scores)
        print(f"  ãƒˆãƒ”ãƒƒã‚¯æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ç¯„å›²: min={topic_norm_arr.min():.6f}, max={topic_norm_arr.max():.6f}, mean={topic_norm_arr.mean():.6f}")
    else:
        print(f"  ãƒˆãƒ”ãƒƒã‚¯æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢: ãƒ‡ãƒ¼ã‚¿ãªã—")

    if final_raw_scores:
        final_arr = np.array(final_raw_scores)
        print(f"  æœ€çµ‚ç”Ÿã‚¹ã‚³ã‚¢ç¯„å›²: min={final_arr.min():.6f}, max={final_arr.max():.6f}, mean={final_arr.mean():.6f}")
    else:
        print(f"  æœ€çµ‚ç”Ÿã‚¹ã‚³ã‚¢: ãƒ‡ãƒ¼ã‚¿ãªã—")

    # sigmoidå¾Œã®ã‚¹ã‚³ã‚¢ç¯„å›²ã‚’è¿½åŠ 
    if final_sigmoid_scores:
        sigmoid_arr = np.array(final_sigmoid_scores)
        print(f"  sigmoidå¾Œæœ€çµ‚ã‚¹ã‚³ã‚¢ç¯„å›²: min={sigmoid_arr.min():.6f}, max={sigmoid_arr.max():.6f}, mean={sigmoid_arr.mean():.6f}")
    else:
        print(f"  sigmoidå¾Œæœ€çµ‚ã‚¹ã‚³ã‚¢: ãƒ‡ãƒ¼ã‚¿ãªã—")
    
    # =======================================================
    # çµæœåˆ†æ
    # =======================================================
    print(f"\næ¨è«–çµæœ:")
    print(f"ç·ã‚¹ã‚³ã‚¢æ•°: {len(scores)}")
    print(f"NaN/Infã®æ•°: {nan_count}")
    print(f"ã‚¹ã‚³ã‚¢çµ±è¨ˆ: min={min(scores):.6f}, max={max(scores):.6f}, mean={np.mean(scores):.6f}")
    
    # æ·±åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
    depth_scores = depth_score_cal(scores)
    depth_array = np.array(depth_scores)
    print(f"æ·±åº¦ã‚¹ã‚³ã‚¢çµ±è¨ˆ: min={depth_array.min():.6f}, max={depth_array.max():.6f}, mean={depth_array.mean():.6f}")
    
    # =======================================================
    # å¢ƒç•Œæ¤œå‡ºã¨è©•ä¾¡
    # =======================================================
    print(f"å¢ƒç•Œæ¤œå‡ºä¸­...")
    
    methods = ['adaptive', 'fixed', 'threshold']
    best_pk = float('inf')
    best_method = None
    best_boundaries = None
    best_seg_pred = None
    best_window_size = None
    
    # ãƒ©ãƒ³ãƒ€ãƒ å¢ƒç•Œæ¤œå‡ºã®è©•ä¾¡ç”¨å¤‰æ•°ã‚’è¿½åŠ 
    random_pk_scores = []
    random_wd_scores = []
    
    for method in methods:
        if method == 'adaptive':
            boundaries = detect_boundaries(depth_scores, method='adaptive')
        elif method == 'fixed':
            boundaries = detect_boundaries(depth_scores, method='fixed', 
                                         num_boundaries=sum(boundary_labels))
        elif method == 'threshold':
            threshold = depth_array.mean() + 0.5 * depth_array.std()
            boundaries = detect_boundaries(depth_scores, method='threshold', threshold=threshold)
        
        seg_pred = [0] * len(sentences)
        for b in boundaries:
            if b < len(seg_pred):
                seg_pred[b] = 1
        
        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        seg_r = []
        tmp = 0
        for g in boundary_labels:
            tmp += 1
            if g == 1:
                seg_r.append(tmp)
                tmp = 0
        if tmp > 0:
            seg_r.append(tmp)
        
        seg_p = []
        tmp = 0
        for p in seg_pred:
            tmp += 1
            if p == 1:
                seg_p.append(tmp)
                tmp = 0
        if tmp > 0:
            seg_p.append(tmp)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºæƒ…å ±ã®è¨ˆç®—
        avg_segment_length = np.mean(seg_r)
        window_size = int(avg_segment_length / 2)
        
        pk = segeval.pk(seg_p, seg_r)
        wd = segeval.window_diff(seg_p, seg_r)
        
        print(f"  {method}: å¢ƒç•Œæ•°={sum(seg_pred)}, Pk={pk:.4f}, WD={wd:.4f}, ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º={window_size}")
        
        if pk < best_pk:
            best_pk = pk
            best_method = method
            best_boundaries = boundaries
            best_seg_pred = seg_pred
            best_window_size = window_size
    
    # ãƒ©ãƒ³ãƒ€ãƒ å¢ƒç•Œæ¤œå‡ºã®è©•ä¾¡ï¼ˆ100å›è©¦è¡Œï¼‰
    print(f"\nãƒ©ãƒ³ãƒ€ãƒ å¢ƒç•Œæ¤œå‡ºè©•ä¾¡ä¸­...")
    num_sentences = len(sentences)
    num_true_boundaries = sum(boundary_labels)
    
    for i in range(100):
        # ãƒ©ãƒ³ãƒ€ãƒ ã«å¢ƒç•Œã‚’é¸æŠ
        random_boundaries = np.random.choice(num_sentences, size=num_true_boundaries, replace=False)
        random_boundaries = sorted(random_boundaries)
        
        seg_pred_random = [0] * num_sentences
        for b in random_boundaries:
            seg_pred_random[b] = 1
        
        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        seg_r = []
        tmp = 0
        for g in boundary_labels:
            tmp += 1
            if g == 1:
                seg_r.append(tmp)
                tmp = 0
        if tmp > 0:
            seg_r.append(tmp)
        
        seg_p = []
        tmp = 0
        for p in seg_pred_random:
            tmp += 1
            if p == 1:
                seg_p.append(tmp)
                tmp = 0
        if tmp > 0:
            seg_p.append(tmp)
        
        pk_random = segeval.pk(seg_p, seg_r)
        wd_random = segeval.window_diff(seg_p, seg_r)
        
        random_pk_scores.append(pk_random)
        random_wd_scores.append(wd_random)
    
    # ãƒ©ãƒ³ãƒ€ãƒ å¢ƒç•Œæ¤œå‡ºã®çµ±è¨ˆã‚’è¨ˆç®—
    random_pk_mean = np.mean(random_pk_scores)
    random_pk_std = np.std(random_pk_scores)
    random_wd_mean = np.mean(random_wd_scores)
    random_wd_std = np.std(random_wd_scores)
    
    print(f"ãƒ©ãƒ³ãƒ€ãƒ å¢ƒç•Œæ¤œå‡ºçµæœ (100å›è©¦è¡Œ):")
    print(f"  Pk: å¹³å‡={random_pk_mean:.4f}, æ¨™æº–åå·®={random_pk_std:.4f}")
    print(f"  WD: å¹³å‡={random_wd_mean:.4f}, æ¨™æº–åå·®={random_wd_std:.4f}")
    
    # æœ€è‰¯ã®çµæœã‚’ä½¿ç”¨
    seg_pred = best_seg_pred
    boundaries = best_boundaries
    
    # è©³ç´°è©•ä¾¡
    correct_detections = 0
    false_positives = 0
    false_negatives = 0
    
    for pred, gold in zip(seg_pred, boundary_labels):
        if pred == 1 and gold == 1:
            correct_detections += 1
        elif pred == 1 and gold == 0:
            false_positives += 1
        elif pred == 0 and gold == 1:
            false_negatives += 1
    
    precision = correct_detections / sum(seg_pred) if sum(seg_pred) > 0 else 0.0
    recall = correct_detections / sum(boundary_labels) if sum(boundary_labels) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    print(f"è©³ç´°è©•ä¾¡:")
    print(f"  é©åˆç‡: {precision:.4f}")
    print(f"  å†ç¾ç‡: {recall:.4f}") 
    print(f"  F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
    print(f"  Pkã‚¹ã‚³ã‚¢: {best_pk:.4f}")
    print(f"  ä½¿ç”¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {best_window_size}")
    
    # çµæœã‚’è¿”ã™
    result = {
        "model_name": model_name,
        "model_config": model_config,
        "Pk": float(best_pk),
        "WD": float(segeval.window_diff(seg_p, seg_r)),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "predicted_boundaries": seg_pred,
        "gold_boundaries": boundary_labels,
        "depth_scores": [float(score) for score in depth_scores],
        "raw_scores": [float(score) for score in scores],
        "sentences": sentences,
        "best_method": best_method,
        "detected_boundary_count": sum(seg_pred),
        "true_boundary_count": sum(boundary_labels),
        "correct_detections": correct_detections,
        "false_positives": false_positives,
        "false_negatives": false_negatives,
        "window_size": int(best_window_size),
        "random_boundary_evaluation": {  # ãƒ©ãƒ³ãƒ€ãƒ å¢ƒç•Œæ¤œå‡ºã®çµæœã‚’è¿½åŠ 
            "pk_mean": float(random_pk_mean),
            "pk_std": float(random_pk_std),
            "wd_mean": float(random_wd_mean),
            "wd_std": float(random_wd_std),
            "num_trials": 100
        },
        "score_statistics": {
            "min": float(min(scores)),
            "max": float(max(scores)),
            "mean": float(np.mean(scores)),
            "std": float(np.std(scores))
        },
        "depth_score_statistics": {
            "min": float(depth_array.min()),
            "max": float(depth_array.max()),
            "mean": float(depth_array.mean()),
            "std": float(depth_array.std())
        },
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚‚ä¿å­˜ï¼ˆå¯è¦–åŒ–ç”¨ã«ç”Ÿã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼‰
        "debug_scores": {
            "coherence_raw_scores": coherence_raw_scores[:len(scores)],  # å¯è¦–åŒ–ç”¨ã«è¿½åŠ 
            "topic_raw_scores": topic_raw_scores[:len(scores)],  # å¯è¦–åŒ–ç”¨ã«è¿½åŠ 
            "coherence_normalized_scores": coherence_normalized_scores[:len(scores)],  # æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
            "topic_normalized_scores": topic_normalized_scores[:len(scores)],  # æ­£è¦åŒ–å¾Œã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
            "coherence_raw_range": {
                "min": float(coherence_arr.min()) if coherence_raw_scores else 0.0,
                "max": float(coherence_arr.max()) if coherence_raw_scores else 0.0,
                "mean": float(coherence_arr.mean()) if coherence_raw_scores else 0.0
            },
            "topic_raw_range": {
                "min": float(topic_arr.min()) if topic_raw_scores else 0.0,
                "max": float(topic_arr.max()) if topic_raw_scores else 0.0,
                "mean": float(topic_arr.mean()) if topic_raw_scores else 0.0
            },
            "final_raw_range": {
                "min": float(final_arr.min()) if final_raw_scores else 0.0,
                "max": float(final_arr.max()) if final_raw_scores else 0.0,
                "mean": float(final_arr.mean()) if final_raw_scores else 0.0
            },
            "sigmoid_final_range": {
                "min": float(sigmoid_arr.min()) if final_sigmoid_scores else 0.0,
                "max": float(sigmoid_arr.max()) if final_sigmoid_scores else 0.0,
                "mean": float(sigmoid_arr.mean()) if final_sigmoid_scores else 0.0
            }
        }
    }
    
    # CSVã«çµæœã‚’ä¿å­˜
    save_results_to_csv(result, SAVE_PATH)
    
    # ã‚¹ã‚³ã‚¢ã‚’å¯è¦–åŒ–
    visualize_scores(result, SAVE_PATH)
    
    return result

def main():
    print("=== å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ¨è«–é–‹å§‹ ===")
    
    all_results = {}
    model_name = "trained-model"
    model_config = MODEL_CONFIGS[model_name]
    
    result = run_inference_for_model(model_name, model_config)
    if result is not None:
        all_results[model_name] = result
        
        # çµæœè¡¨ç¤º
        print(f"\n{'='*60}")
        print("æ¨è«–çµæœã‚µãƒãƒªãƒ¼")
        print(f"{'='*60}")
        print(f"ãƒ¢ãƒ‡ãƒ«å: {model_name}")
        print(f"Pkã‚¹ã‚³ã‚¢: {result['Pk']:.4f}")
        print(f"F1ã‚¹ã‚³ã‚¢: {result['f1_score']:.4f}")
        print(f"é©åˆç‡: {result['precision']:.4f}")
        print(f"å†ç¾ç‡: {result['recall']:.4f}")
        print(f"æ¤œå‡ºå¢ƒç•Œæ•°: {result['detected_boundary_count']}")
        print(f"æ­£è§£å¢ƒç•Œæ•°: {result['true_boundary_count']}")
        print(f"æœ€é©ãªå¢ƒç•Œæ¤œå‡ºæ–¹æ³•: {result['best_method']}")
        print(f"ä½¿ç”¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {result['window_size']}")
        print(f"ã‚³ãƒ¡ãƒ³ãƒˆä½¿ç”¨ãƒ•ãƒ©ã‚°: {model_config.get('use_comments_for_topic', True)}")
        print(f"èåˆæ–¹æ³•: {model_config.get('fusion_method', 'average')}")
    
    # çµæœä¿å­˜
    final_results = {
        "model_results": all_results,
        "timestamp": str(np.datetime64('now'))
    }
    
    # çµæœã‚’ä¿å­˜
    result_save_path = f"{SAVE_PATH}/trained_model_results.json"
    with open(result_save_path, "w", encoding="utf-8") as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False, cls=CustomJSONEncoder)
    
    print(f"\nçµæœã‚’ä¿å­˜: {result_save_path}")
    print("=== æ¨è«–å®Œäº† ===")

if __name__ == "__main__":
    set_seed(3407)
    main()